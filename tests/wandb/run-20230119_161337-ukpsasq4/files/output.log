
0it [00:00, ?it/s]/home/ubuntu/.local/lib/python3.8/site-packages/jax/interpreters/mlir.py:675: UserWarning: Some donated buffers were not usable: ShapedArray(int32[]), ShapedArray(uint8[16,384,384,3]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
0it [24:27, ?it/s]
Traceback (most recent call last):
  File "train.py", line 258, in <module>
    app()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/typer/main.py", line 311, in __call__
    return get_command(self)(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/typer/core.py", line 716, in main
    return _main(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/typer/core.py", line 216, in _main
    rv = self.invoke(ctx)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/typer/main.py", line 683, in wrapper
    return callback(**use_params)  # type: ignore
  File "train.py", line 238, in main
    state, scalars = p_train_step(state, batch)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/_src/traceback_util.py", line 162, in reraise_with_filtered_traceback
    return fun(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/_src/api.py", line 2253, in cache_miss
    execute = pxla.xla_pmap_impl_lazy(fun_, *tracers, **params)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/interpreters/pxla.py", line 974, in xla_pmap_impl_lazy
    compiled_fun, fingerprint = parallel_callable(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/linear_util.py", line 303, in memoized_fun
    ans = call(fun, *args)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/interpreters/pxla.py", line 1248, in parallel_callable
    pmap_executable = pmap_computation.compile()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/_src/profiler.py", line 314, in wrapper
    return func(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/interpreters/pxla.py", line 1535, in compile
    executable = self._compile_unloaded()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/interpreters/pxla.py", line 1512, in _compile_unloaded
    return UnloadedPmapExecutable.from_hlo(self._hlo, **self.compile_args)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/interpreters/pxla.py", line 1673, in from_hlo
    compiled = dispatch.compile_or_get_cached(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/_src/dispatch.py", line 1079, in compile_or_get_cached
    return backend_compile(backend, serialized_computation, compile_options,
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/_src/profiler.py", line 314, in wrapper
    return func(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/_src/dispatch.py", line 1014, in backend_compile
    return backend.compile(built_c, compile_options=options)
jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 58.81G of 30.75G hbm. Exceeded hbm capacity by 28.06G.
Total hbm usage >= 60.06G:
    reserved          1.25G
    program          58.81G
    arguments            0B
Output size 0B; shares 0B with arguments.
Program hbm requirement 58.81G:
    global           468.0K
    scoped           781.0K
    HLO temp         58.81G (99.9% utilization: Unpadded (57.26G) Padded (57.29G), 2.6% fragmentation (1.52G))
  Largest program allocations in hbm:
  1. Size: 2.30G
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.decode/decoder/up_blocks_2/upsamplers_0/conv/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 256) rhs_shape=(3, 3, 3, 256, 256) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,256]{4,2,3,1,0:T(8,128)}
     Unpadded size: 2.30G
     XLA label: fusion.133.remat6 = fusion(copy-done.816, slice.893.remat3, copy.2752, copy-done.3648, ...(+1)), kind=kOutput, calls=fused_computation.124.clone.clone.clone.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  2. Size: 1.72G
     Shape: bf16[16,384,8,49,128,3]{4,2,3,1,0,5:T(8,128)(2,1)}
     Unpadded size: 1.72G
     XLA label: fusion.78 = fusion(fusion.554, get-tuple-element.8060, fusion.553), kind=kLoop, calls=fused_computation.78
     Allocation type: HLO temp
     ==========================
  3. Size: 1.17G
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.decode/decoder/up_blocks_1/upsamplers_0/conv/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 192, 192, 512) rhs_shape=(3, 3, 3, 512, 512) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,192,8,25,512]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.17G
     XLA label: fusion.435.remat8 = fusion(copy-done.531, get-tuple-element.8175, copy.2718, copy-done.3290, ...(+1)), kind=kOutput, calls=fused_computation.373.clone.clone.clone.clone.clone.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  4. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.encode/encoder/down_blocks_0/resnets_1/conv2/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.707.remat = fusion(copy-done.668, copy-done.2770, fusion.488.remat6, copy-done.2769, ...(+5)), kind=kOutput, calls=fused_computation.615.clone
     Allocation type: HLO temp
     ==========================
  5. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.encode/encoder/down_blocks_0/resnets_0/conv1/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.494.remat6 = fusion(copy-done.1037, copy-done.407, get-tuple-element.5682, copy-done.411, ...(+1)), kind=kOutput, calls=fused_computation.408.clone.clone.clone.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  6. Size: 1.15G
     Shape: bf16[16,384,8,49,256,1]{4,2,3,1,0,5:T(8,128)(2,1)}
     Unpadded size: 1.15G
     XLA label: fusion.728.remat2 = fusion(Arg_119.120, copy-done.1325, fusion.133.remat6, copy-done.1321, ...(+2)), kind=kLoop, calls=fused_computation.636.clone.clone
     Allocation type: HLO temp
     ==========================
  7. Size: 1.15G
     Shape: bf16[16,384,1,8,49,256]{5,3,4,1,0,2:T(8,128)(2,1)}
     Unpadded size: 1.15G
     XLA label: fusion.728.remat2 = fusion(Arg_119.120, copy-done.1325, fusion.133.remat6, copy-done.1321, ...(+2)), kind=kLoop, calls=fused_computation.636.clone.clone
     Allocation type: HLO temp
     ==========================
  8. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.encode/encoder/conv_in/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 3) rhs_shape=(3, 3, 3, 3, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.615.remat6 = fusion(copy.2492.remat_uncompressed, copy-done.1161, copy-done.2808, copy-done.656), kind=kOutput, calls=fused_computation.523.clone.clone.clone.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  9. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/conv_out/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 3) rhs_shape=(3, 3, 3, 128, 3) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.717.remat3.1.remat2 = fusion(copy-done.660, copy-done.1927, get-tuple-element.8152, copy-done.1925, ...(+5)), kind=kOutput, calls=fused_computation.625.clone.clone.clone.1.clone.clone
     Allocation type: HLO temp
     ==========================
  10. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_0/conv_shortcut/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (0, 0), (0, 0)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 1, 1, 256, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: bf16[16,384,8,49,256]{4,2,3,1,0:T(8,128)(2,1)}
     Unpadded size: 1.15G
     XLA label: fusion.6494 = fusion(copy-done.1958, fusion.133.remat6, copy-done.1957, fusion.279.remat4, ...(+6)), kind=kOutput, calls=fused_computation.5283
     Allocation type: HLO temp
     ==========================
  11. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_0/conv2/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.709.remat2 = fusion(copy-done.665, copy.2830, get-tuple-element.8051, copy-done.2773, ...(+7)), kind=kOutput, calls=fused_computation.617.clone.clone
     Allocation type: HLO temp
     ==========================
  12. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_1/conv1/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.724.remat2 = fusion(copy-done.664, copy.2822, get-tuple-element.5974, copy-done.2772, ...(+8)), kind=kOutput, calls=fused_computation.632.clone.clone
     Allocation type: HLO temp
     ==========================
  13. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_1/conv2/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.711.remat2 = fusion(copy-done.663, copy.2815, get-tuple-element.8101, copy-done.2771, ...(+7)), kind=kOutput, calls=fused_computation.619.clone.clone
     Allocation type: HLO temp
     ==========================
  14. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_2/conv1/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.713.remat2 = fusion(copy-done.662, copy.2807, get-tuple-element.8120, copy.2805, ...(+8)), kind=kOutput, calls=fused_computation.621.clone.clone
     Allocation type: HLO temp
     ==========================
  15. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_2/conv2/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.715.remat = fusion(copy-done.661, copy.2800, fusion.470.remat6, copy-done.2774, ...(+7)), kind=kOutput, calls=fused_computation.623.clone
     Allocation type: HLO temp
     ==========================
  16. Size: 1.12G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.encode/encoder/down_blocks_0/resnets_0/conv2/reduce_sum[axes=(1, 2, 3)]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=464
     Shape: f32[16,384,384,128]{3,1,2,0:T(8,128)}
     Unpadded size: 1.12G
     XLA label: fusion.6248 = fusion(copy-done.2061, get-tuple-element.4819, copy-done.2059, copy-done.2553, ...(+4)), kind=kLoop, calls=fused_computation.5100
     Allocation type: HLO temp
     ==========================
  17. Size: 1.12G
     Shape: bf16[16,384,1,384,256]{4,1,0,3,2:T(8,128)(2,1)}
     Unpadded size: 1.12G
     XLA label: fusion.6950 = fusion(bitcast.277, bitcast.276, bitcast.275), kind=kLoop, calls=fused_computation.152.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  18. Size: 612.00M
     Shape: bf16[16,384,8,51,128]{4,1,3,2,0:T(8,128)(2,1)}
     Unpadded size: 612.00M
     XLA label: fusion.263.remat7 = fusion(copy-done.207, bitcast.4734, bitcast.4733), kind=kLoop, calls=fused_computation.243.clone.clone.clone.clone.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  19. Size: 600.00M
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.decode/decoder/up_blocks_2/resnets_1/conv1/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 192, 192, 256) rhs_shape=(3, 3, 3, 256, 256) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,192,8,25,256]{4,2,3,1,0:T(8,128)}
     Unpadded size: 600.00M
     XLA label: fusion.1106.remat3 = fusion(copy-done.962, copy-done.2560, copy-done.2558, fusion.950.remat5, ...(+8)), kind=kOutput, calls=fused_computation.950.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  20. Size: 600.00M
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.encode/encoder/down_blocks_1/resnets_0/conv1/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 192, 192, 128) rhs_shape=(3, 3, 3, 128, 256) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,192,8,25,256]{4,2,3,1,0:T(8,128)}
     Unpadded size: 600.00M
     XLA label: fusion.1100.remat2 = fusion(copy-done.970, copy-done.2564, copy-done.2562, fusion.959.remat5, ...(+8)), kind=kOutput, calls=fused_computation.944.clone.clone
     Allocation type: HLO temp
     ==========================
The stack trace below excludes JAX-internal frames.
The preceding is the original exception that occurred, unmodified.
--------------------
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "train.py", line 258, in <module>
    app()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/typer/main.py", line 328, in __call__
    raise e
  File "/home/ubuntu/.local/lib/python3.8/site-packages/typer/main.py", line 311, in __call__
    return get_command(self)(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/typer/core.py", line 716, in main
    return _main(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/typer/core.py", line 216, in _main
    rv = self.invoke(ctx)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/typer/main.py", line 683, in wrapper
    return callback(**use_params)  # type: ignore
  File "train.py", line 238, in main
    state, scalars = p_train_step(state, batch)
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 58.81G of 30.75G hbm. Exceeded hbm capacity by 28.06G.
Total hbm usage >= 60.06G:
    reserved          1.25G
    program          58.81G
    arguments            0B
Output size 0B; shares 0B with arguments.
Program hbm requirement 58.81G:
    global           468.0K
    scoped           781.0K
    HLO temp         58.81G (99.9% utilization: Unpadded (57.26G) Padded (57.29G), 2.6% fragmentation (1.52G))
  Largest program allocations in hbm:
  1. Size: 2.30G
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.decode/decoder/up_blocks_2/upsamplers_0/conv/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 256) rhs_shape=(3, 3, 3, 256, 256) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,256]{4,2,3,1,0:T(8,128)}
     Unpadded size: 2.30G
     XLA label: fusion.133.remat6 = fusion(copy-done.816, slice.893.remat3, copy.2752, copy-done.3648, ...(+1)), kind=kOutput, calls=fused_computation.124.clone.clone.clone.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  2. Size: 1.72G
     Shape: bf16[16,384,8,49,128,3]{4,2,3,1,0,5:T(8,128)(2,1)}
     Unpadded size: 1.72G
     XLA label: fusion.78 = fusion(fusion.554, get-tuple-element.8060, fusion.553), kind=kLoop, calls=fused_computation.78
     Allocation type: HLO temp
     ==========================
  3. Size: 1.17G
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.decode/decoder/up_blocks_1/upsamplers_0/conv/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 192, 192, 512) rhs_shape=(3, 3, 3, 512, 512) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,192,8,25,512]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.17G
     XLA label: fusion.435.remat8 = fusion(copy-done.531, get-tuple-element.8175, copy.2718, copy-done.3290, ...(+1)), kind=kOutput, calls=fused_computation.373.clone.clone.clone.clone.clone.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  4. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.encode/encoder/down_blocks_0/resnets_1/conv2/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.707.remat = fusion(copy-done.668, copy-done.2770, fusion.488.remat6, copy-done.2769, ...(+5)), kind=kOutput, calls=fused_computation.615.clone
     Allocation type: HLO temp
     ==========================
  5. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.encode/encoder/down_blocks_0/resnets_0/conv1/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.494.remat6 = fusion(copy-done.1037, copy-done.407, get-tuple-element.5682, copy-done.411, ...(+1)), kind=kOutput, calls=fused_computation.408.clone.clone.clone.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  6. Size: 1.15G
     Shape: bf16[16,384,8,49,256,1]{4,2,3,1,0,5:T(8,128)(2,1)}
     Unpadded size: 1.15G
     XLA label: fusion.728.remat2 = fusion(Arg_119.120, copy-done.1325, fusion.133.remat6, copy-done.1321, ...(+2)), kind=kLoop, calls=fused_computation.636.clone.clone
     Allocation type: HLO temp
     ==========================
  7. Size: 1.15G
     Shape: bf16[16,384,1,8,49,256]{5,3,4,1,0,2:T(8,128)(2,1)}
     Unpadded size: 1.15G
     XLA label: fusion.728.remat2 = fusion(Arg_119.120, copy-done.1325, fusion.133.remat6, copy-done.1321, ...(+2)), kind=kLoop, calls=fused_computation.636.clone.clone
     Allocation type: HLO temp
     ==========================
  8. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.encode/encoder/conv_in/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 3) rhs_shape=(3, 3, 3, 3, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.615.remat6 = fusion(copy.2492.remat_uncompressed, copy-done.1161, copy-done.2808, copy-done.656), kind=kOutput, calls=fused_computation.523.clone.clone.clone.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  9. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/conv_out/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 3) rhs_shape=(3, 3, 3, 128, 3) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.717.remat3.1.remat2 = fusion(copy-done.660, copy-done.1927, get-tuple-element.8152, copy-done.1925, ...(+5)), kind=kOutput, calls=fused_computation.625.clone.clone.clone.1.clone.clone
     Allocation type: HLO temp
     ==========================
  10. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_0/conv_shortcut/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (0, 0), (0, 0)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 1, 1, 256, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: bf16[16,384,8,49,256]{4,2,3,1,0:T(8,128)(2,1)}
     Unpadded size: 1.15G
     XLA label: fusion.6494 = fusion(copy-done.1958, fusion.133.remat6, copy-done.1957, fusion.279.remat4, ...(+6)), kind=kOutput, calls=fused_computation.5283
     Allocation type: HLO temp
     ==========================
  11. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_0/conv2/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.709.remat2 = fusion(copy-done.665, copy.2830, get-tuple-element.8051, copy-done.2773, ...(+7)), kind=kOutput, calls=fused_computation.617.clone.clone
     Allocation type: HLO temp
     ==========================
  12. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_1/conv1/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.724.remat2 = fusion(copy-done.664, copy.2822, get-tuple-element.5974, copy-done.2772, ...(+8)), kind=kOutput, calls=fused_computation.632.clone.clone
     Allocation type: HLO temp
     ==========================
  13. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_1/conv2/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.711.remat2 = fusion(copy-done.663, copy.2815, get-tuple-element.8101, copy-done.2771, ...(+7)), kind=kOutput, calls=fused_computation.619.clone.clone
     Allocation type: HLO temp
     ==========================
  14. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_2/conv1/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.713.remat2 = fusion(copy-done.662, copy.2807, get-tuple-element.8120, copy.2805, ...(+8)), kind=kOutput, calls=fused_computation.621.clone.clone
     Allocation type: HLO temp
     ==========================
  15. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_2/conv2/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.715.remat = fusion(copy-done.661, copy.2800, fusion.470.remat6, copy-done.2774, ...(+7)), kind=kOutput, calls=fused_computation.623.clone
     Allocation type: HLO temp
     ==========================
  16. Size: 1.12G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.encode/encoder/down_blocks_0/resnets_0/conv2/reduce_sum[axes=(1, 2, 3)]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=464
     Shape: f32[16,384,384,128]{3,1,2,0:T(8,128)}
     Unpadded size: 1.12G
     XLA label: fusion.6248 = fusion(copy-done.2061, get-tuple-element.4819, copy-done.2059, copy-done.2553, ...(+4)), kind=kLoop, calls=fused_computation.5100
     Allocation type: HLO temp
     ==========================
  17. Size: 1.12G
     Shape: bf16[16,384,1,384,256]{4,1,0,3,2:T(8,128)(2,1)}
     Unpadded size: 1.12G
     XLA label: fusion.6950 = fusion(bitcast.277, bitcast.276, bitcast.275), kind=kLoop, calls=fused_computation.152.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  18. Size: 612.00M
     Shape: bf16[16,384,8,51,128]{4,1,3,2,0:T(8,128)(2,1)}
     Unpadded size: 612.00M
     XLA label: fusion.263.remat7 = fusion(copy-done.207, bitcast.4734, bitcast.4733), kind=kLoop, calls=fused_computation.243.clone.clone.clone.clone.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  19. Size: 600.00M
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.decode/decoder/up_blocks_2/resnets_1/conv1/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 192, 192, 256) rhs_shape=(3, 3, 3, 256, 256) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,192,8,25,256]{4,2,3,1,0:T(8,128)}
     Unpadded size: 600.00M
     XLA label: fusion.1106.remat3 = fusion(copy-done.962, copy-done.2560, copy-done.2558, fusion.950.remat5, ...(+8)), kind=kOutput, calls=fused_computation.950.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  20. Size: 600.00M
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.encode/encoder/down_blocks_1/resnets_0/conv1/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 192, 192, 128) rhs_shape=(3, 3, 3, 128, 256) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,192,8,25,256]{4,2,3,1,0:T(8,128)}
     Unpadded size: 600.00M
     XLA label: fusion.1100.remat2 = fusion(copy-done.970, copy-done.2564, copy-done.2562, fusion.959.remat5, ...(+8)), kind=kOutput, calls=fused_computation.944.clone.clone
     Allocation type: HLO temp
     ==========================
Traceback (most recent call last):
  File "train.py", line 258, in <module>
    app()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/typer/main.py", line 311, in __call__
    return get_command(self)(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/typer/core.py", line 716, in main
    return _main(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/typer/core.py", line 216, in _main
    rv = self.invoke(ctx)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/typer/main.py", line 683, in wrapper
    return callback(**use_params)  # type: ignore
  File "train.py", line 238, in main
    state, scalars = p_train_step(state, batch)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/_src/traceback_util.py", line 162, in reraise_with_filtered_traceback
    return fun(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/_src/api.py", line 2253, in cache_miss
    execute = pxla.xla_pmap_impl_lazy(fun_, *tracers, **params)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/interpreters/pxla.py", line 974, in xla_pmap_impl_lazy
    compiled_fun, fingerprint = parallel_callable(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/linear_util.py", line 303, in memoized_fun
    ans = call(fun, *args)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/interpreters/pxla.py", line 1248, in parallel_callable
    pmap_executable = pmap_computation.compile()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/_src/profiler.py", line 314, in wrapper
    return func(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/interpreters/pxla.py", line 1535, in compile
    executable = self._compile_unloaded()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/interpreters/pxla.py", line 1512, in _compile_unloaded
    return UnloadedPmapExecutable.from_hlo(self._hlo, **self.compile_args)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/interpreters/pxla.py", line 1673, in from_hlo
    compiled = dispatch.compile_or_get_cached(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/_src/dispatch.py", line 1079, in compile_or_get_cached
    return backend_compile(backend, serialized_computation, compile_options,
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/_src/profiler.py", line 314, in wrapper
    return func(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/jax/_src/dispatch.py", line 1014, in backend_compile
    return backend.compile(built_c, compile_options=options)
jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 58.81G of 30.75G hbm. Exceeded hbm capacity by 28.06G.
Total hbm usage >= 60.06G:
    reserved          1.25G
    program          58.81G
    arguments            0B
Output size 0B; shares 0B with arguments.
Program hbm requirement 58.81G:
    global           468.0K
    scoped           781.0K
    HLO temp         58.81G (99.9% utilization: Unpadded (57.26G) Padded (57.29G), 2.6% fragmentation (1.52G))
  Largest program allocations in hbm:
  1. Size: 2.30G
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.decode/decoder/up_blocks_2/upsamplers_0/conv/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 256) rhs_shape=(3, 3, 3, 256, 256) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,256]{4,2,3,1,0:T(8,128)}
     Unpadded size: 2.30G
     XLA label: fusion.133.remat6 = fusion(copy-done.816, slice.893.remat3, copy.2752, copy-done.3648, ...(+1)), kind=kOutput, calls=fused_computation.124.clone.clone.clone.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  2. Size: 1.72G
     Shape: bf16[16,384,8,49,128,3]{4,2,3,1,0,5:T(8,128)(2,1)}
     Unpadded size: 1.72G
     XLA label: fusion.78 = fusion(fusion.554, get-tuple-element.8060, fusion.553), kind=kLoop, calls=fused_computation.78
     Allocation type: HLO temp
     ==========================
  3. Size: 1.17G
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.decode/decoder/up_blocks_1/upsamplers_0/conv/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 192, 192, 512) rhs_shape=(3, 3, 3, 512, 512) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,192,8,25,512]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.17G
     XLA label: fusion.435.remat8 = fusion(copy-done.531, get-tuple-element.8175, copy.2718, copy-done.3290, ...(+1)), kind=kOutput, calls=fused_computation.373.clone.clone.clone.clone.clone.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  4. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.encode/encoder/down_blocks_0/resnets_1/conv2/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.707.remat = fusion(copy-done.668, copy-done.2770, fusion.488.remat6, copy-done.2769, ...(+5)), kind=kOutput, calls=fused_computation.615.clone
     Allocation type: HLO temp
     ==========================
  5. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.encode/encoder/down_blocks_0/resnets_0/conv1/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.494.remat6 = fusion(copy-done.1037, copy-done.407, get-tuple-element.5682, copy-done.411, ...(+1)), kind=kOutput, calls=fused_computation.408.clone.clone.clone.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  6. Size: 1.15G
     Shape: bf16[16,384,8,49,256,1]{4,2,3,1,0,5:T(8,128)(2,1)}
     Unpadded size: 1.15G
     XLA label: fusion.728.remat2 = fusion(Arg_119.120, copy-done.1325, fusion.133.remat6, copy-done.1321, ...(+2)), kind=kLoop, calls=fused_computation.636.clone.clone
     Allocation type: HLO temp
     ==========================
  7. Size: 1.15G
     Shape: bf16[16,384,1,8,49,256]{5,3,4,1,0,2:T(8,128)(2,1)}
     Unpadded size: 1.15G
     XLA label: fusion.728.remat2 = fusion(Arg_119.120, copy-done.1325, fusion.133.remat6, copy-done.1321, ...(+2)), kind=kLoop, calls=fused_computation.636.clone.clone
     Allocation type: HLO temp
     ==========================
  8. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.encode/encoder/conv_in/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 3) rhs_shape=(3, 3, 3, 3, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.615.remat6 = fusion(copy.2492.remat_uncompressed, copy-done.1161, copy-done.2808, copy-done.656), kind=kOutput, calls=fused_computation.523.clone.clone.clone.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  9. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/conv_out/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 3) rhs_shape=(3, 3, 3, 128, 3) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.717.remat3.1.remat2 = fusion(copy-done.660, copy-done.1927, get-tuple-element.8152, copy-done.1925, ...(+5)), kind=kOutput, calls=fused_computation.625.clone.clone.clone.1.clone.clone
     Allocation type: HLO temp
     ==========================
  10. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_0/conv_shortcut/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (0, 0), (0, 0)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 1, 1, 256, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: bf16[16,384,8,49,256]{4,2,3,1,0:T(8,128)(2,1)}
     Unpadded size: 1.15G
     XLA label: fusion.6494 = fusion(copy-done.1958, fusion.133.remat6, copy-done.1957, fusion.279.remat4, ...(+6)), kind=kOutput, calls=fused_computation.5283
     Allocation type: HLO temp
     ==========================
  11. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_0/conv2/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.709.remat2 = fusion(copy-done.665, copy.2830, get-tuple-element.8051, copy-done.2773, ...(+7)), kind=kOutput, calls=fused_computation.617.clone.clone
     Allocation type: HLO temp
     ==========================
  12. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_1/conv1/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.724.remat2 = fusion(copy-done.664, copy.2822, get-tuple-element.5974, copy-done.2772, ...(+8)), kind=kOutput, calls=fused_computation.632.clone.clone
     Allocation type: HLO temp
     ==========================
  13. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_1/conv2/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.711.remat2 = fusion(copy-done.663, copy.2815, get-tuple-element.8101, copy-done.2771, ...(+7)), kind=kOutput, calls=fused_computation.619.clone.clone
     Allocation type: HLO temp
     ==========================
  14. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_2/conv1/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.713.remat2 = fusion(copy-done.662, copy.2807, get-tuple-element.8120, copy.2805, ...(+8)), kind=kOutput, calls=fused_computation.621.clone.clone
     Allocation type: HLO temp
     ==========================
  15. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_2/conv2/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.715.remat = fusion(copy-done.661, copy.2800, fusion.470.remat6, copy-done.2774, ...(+7)), kind=kOutput, calls=fused_computation.623.clone
     Allocation type: HLO temp
     ==========================
  16. Size: 1.12G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.encode/encoder/down_blocks_0/resnets_0/conv2/reduce_sum[axes=(1, 2, 3)]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=464
     Shape: f32[16,384,384,128]{3,1,2,0:T(8,128)}
     Unpadded size: 1.12G
     XLA label: fusion.6248 = fusion(copy-done.2061, get-tuple-element.4819, copy-done.2059, copy-done.2553, ...(+4)), kind=kLoop, calls=fused_computation.5100
     Allocation type: HLO temp
     ==========================
  17. Size: 1.12G
     Shape: bf16[16,384,1,384,256]{4,1,0,3,2:T(8,128)(2,1)}
     Unpadded size: 1.12G
     XLA label: fusion.6950 = fusion(bitcast.277, bitcast.276, bitcast.275), kind=kLoop, calls=fused_computation.152.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  18. Size: 612.00M
     Shape: bf16[16,384,8,51,128]{4,1,3,2,0:T(8,128)(2,1)}
     Unpadded size: 612.00M
     XLA label: fusion.263.remat7 = fusion(copy-done.207, bitcast.4734, bitcast.4733), kind=kLoop, calls=fused_computation.243.clone.clone.clone.clone.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  19. Size: 600.00M
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.decode/decoder/up_blocks_2/resnets_1/conv1/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 192, 192, 256) rhs_shape=(3, 3, 3, 256, 256) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,192,8,25,256]{4,2,3,1,0:T(8,128)}
     Unpadded size: 600.00M
     XLA label: fusion.1106.remat3 = fusion(copy-done.962, copy-done.2560, copy-done.2558, fusion.950.remat5, ...(+8)), kind=kOutput, calls=fused_computation.950.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  20. Size: 600.00M
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.encode/encoder/down_blocks_1/resnets_0/conv1/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 192, 192, 128) rhs_shape=(3, 3, 3, 128, 256) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,192,8,25,256]{4,2,3,1,0:T(8,128)}
     Unpadded size: 600.00M
     XLA label: fusion.1100.remat2 = fusion(copy-done.970, copy-done.2564, copy-done.2562, fusion.959.remat5, ...(+8)), kind=kOutput, calls=fused_computation.944.clone.clone
     Allocation type: HLO temp
     ==========================
The stack trace below excludes JAX-internal frames.
The preceding is the original exception that occurred, unmodified.
--------------------
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "train.py", line 258, in <module>
    app()
  File "/home/ubuntu/.local/lib/python3.8/site-packages/typer/main.py", line 328, in __call__
    raise e
  File "/home/ubuntu/.local/lib/python3.8/site-packages/typer/main.py", line 311, in __call__
    return get_command(self)(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/typer/core.py", line 716, in main
    return _main(
  File "/home/ubuntu/.local/lib/python3.8/site-packages/typer/core.py", line 216, in _main
    rv = self.invoke(ctx)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/typer/main.py", line 683, in wrapper
    return callback(**use_params)  # type: ignore
  File "train.py", line 238, in main
    state, scalars = p_train_step(state, batch)
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 58.81G of 30.75G hbm. Exceeded hbm capacity by 28.06G.
Total hbm usage >= 60.06G:
    reserved          1.25G
    program          58.81G
    arguments            0B
Output size 0B; shares 0B with arguments.
Program hbm requirement 58.81G:
    global           468.0K
    scoped           781.0K
    HLO temp         58.81G (99.9% utilization: Unpadded (57.26G) Padded (57.29G), 2.6% fragmentation (1.52G))
  Largest program allocations in hbm:
  1. Size: 2.30G
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.decode/decoder/up_blocks_2/upsamplers_0/conv/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 256) rhs_shape=(3, 3, 3, 256, 256) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,256]{4,2,3,1,0:T(8,128)}
     Unpadded size: 2.30G
     XLA label: fusion.133.remat6 = fusion(copy-done.816, slice.893.remat3, copy.2752, copy-done.3648, ...(+1)), kind=kOutput, calls=fused_computation.124.clone.clone.clone.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  2. Size: 1.72G
     Shape: bf16[16,384,8,49,128,3]{4,2,3,1,0,5:T(8,128)(2,1)}
     Unpadded size: 1.72G
     XLA label: fusion.78 = fusion(fusion.554, get-tuple-element.8060, fusion.553), kind=kLoop, calls=fused_computation.78
     Allocation type: HLO temp
     ==========================
  3. Size: 1.17G
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.decode/decoder/up_blocks_1/upsamplers_0/conv/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 192, 192, 512) rhs_shape=(3, 3, 3, 512, 512) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,192,8,25,512]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.17G
     XLA label: fusion.435.remat8 = fusion(copy-done.531, get-tuple-element.8175, copy.2718, copy-done.3290, ...(+1)), kind=kOutput, calls=fused_computation.373.clone.clone.clone.clone.clone.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  4. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.encode/encoder/down_blocks_0/resnets_1/conv2/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.707.remat = fusion(copy-done.668, copy-done.2770, fusion.488.remat6, copy-done.2769, ...(+5)), kind=kOutput, calls=fused_computation.615.clone
     Allocation type: HLO temp
     ==========================
  5. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.encode/encoder/down_blocks_0/resnets_0/conv1/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.494.remat6 = fusion(copy-done.1037, copy-done.407, get-tuple-element.5682, copy-done.411, ...(+1)), kind=kOutput, calls=fused_computation.408.clone.clone.clone.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  6. Size: 1.15G
     Shape: bf16[16,384,8,49,256,1]{4,2,3,1,0,5:T(8,128)(2,1)}
     Unpadded size: 1.15G
     XLA label: fusion.728.remat2 = fusion(Arg_119.120, copy-done.1325, fusion.133.remat6, copy-done.1321, ...(+2)), kind=kLoop, calls=fused_computation.636.clone.clone
     Allocation type: HLO temp
     ==========================
  7. Size: 1.15G
     Shape: bf16[16,384,1,8,49,256]{5,3,4,1,0,2:T(8,128)(2,1)}
     Unpadded size: 1.15G
     XLA label: fusion.728.remat2 = fusion(Arg_119.120, copy-done.1325, fusion.133.remat6, copy-done.1321, ...(+2)), kind=kLoop, calls=fused_computation.636.clone.clone
     Allocation type: HLO temp
     ==========================
  8. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.encode/encoder/conv_in/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 3) rhs_shape=(3, 3, 3, 3, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.615.remat6 = fusion(copy.2492.remat_uncompressed, copy-done.1161, copy-done.2808, copy-done.656), kind=kOutput, calls=fused_computation.523.clone.clone.clone.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  9. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/conv_out/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 3) rhs_shape=(3, 3, 3, 128, 3) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.717.remat3.1.remat2 = fusion(copy-done.660, copy-done.1927, get-tuple-element.8152, copy-done.1925, ...(+5)), kind=kOutput, calls=fused_computation.625.clone.clone.clone.1.clone.clone
     Allocation type: HLO temp
     ==========================
  10. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_0/conv_shortcut/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (0, 0), (0, 0)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 1, 1, 256, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: bf16[16,384,8,49,256]{4,2,3,1,0:T(8,128)(2,1)}
     Unpadded size: 1.15G
     XLA label: fusion.6494 = fusion(copy-done.1958, fusion.133.remat6, copy-done.1957, fusion.279.remat4, ...(+6)), kind=kOutput, calls=fused_computation.5283
     Allocation type: HLO temp
     ==========================
  11. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_0/conv2/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.709.remat2 = fusion(copy-done.665, copy.2830, get-tuple-element.8051, copy-done.2773, ...(+7)), kind=kOutput, calls=fused_computation.617.clone.clone
     Allocation type: HLO temp
     ==========================
  12. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_1/conv1/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.724.remat2 = fusion(copy-done.664, copy.2822, get-tuple-element.5974, copy-done.2772, ...(+8)), kind=kOutput, calls=fused_computation.632.clone.clone
     Allocation type: HLO temp
     ==========================
  13. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_1/conv2/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.711.remat2 = fusion(copy-done.663, copy.2815, get-tuple-element.8101, copy-done.2771, ...(+7)), kind=kOutput, calls=fused_computation.619.clone.clone
     Allocation type: HLO temp
     ==========================
  14. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_2/conv1/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.713.remat2 = fusion(copy-done.662, copy.2807, get-tuple-element.8120, copy.2805, ...(+8)), kind=kOutput, calls=fused_computation.621.clone.clone
     Allocation type: HLO temp
     ==========================
  15. Size: 1.15G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_2/conv2/conv_general_dilated[window_strides=(1, 1, 1) padding=((0, 2), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(3, 4, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 384, 384, 128) rhs_shape=(3, 3, 3, 128, 128) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,384,8,49,128]{4,2,3,1,0:T(8,128)}
     Unpadded size: 1.15G
     XLA label: fusion.715.remat = fusion(copy-done.661, copy.2800, fusion.470.remat6, copy-done.2774, ...(+7)), kind=kOutput, calls=fused_computation.623.clone
     Allocation type: HLO temp
     ==========================
  16. Size: 1.12G
     Operator: op_name="pmap(train_step)/jit(main)/transpose(jvp(FlaxAutoencoderKL))/FlaxAutoencoderKL.encode/encoder/down_blocks_0/resnets_0/conv2/reduce_sum[axes=(1, 2, 3)]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=464
     Shape: f32[16,384,384,128]{3,1,2,0:T(8,128)}
     Unpadded size: 1.12G
     XLA label: fusion.6248 = fusion(copy-done.2061, get-tuple-element.4819, copy-done.2059, copy-done.2553, ...(+4)), kind=kLoop, calls=fused_computation.5100
     Allocation type: HLO temp
     ==========================
  17. Size: 1.12G
     Shape: bf16[16,384,1,384,256]{4,1,0,3,2:T(8,128)(2,1)}
     Unpadded size: 1.12G
     XLA label: fusion.6950 = fusion(bitcast.277, bitcast.276, bitcast.275), kind=kLoop, calls=fused_computation.152.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  18. Size: 612.00M
     Shape: bf16[16,384,8,51,128]{4,1,3,2,0:T(8,128)(2,1)}
     Unpadded size: 612.00M
     XLA label: fusion.263.remat7 = fusion(copy-done.207, bitcast.4734, bitcast.4733), kind=kLoop, calls=fused_computation.243.clone.clone.clone.clone.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  19. Size: 600.00M
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.decode/decoder/up_blocks_2/resnets_1/conv1/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 192, 192, 256) rhs_shape=(3, 3, 3, 256, 256) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,192,8,25,256]{4,2,3,1,0:T(8,128)}
     Unpadded size: 600.00M
     XLA label: fusion.1106.remat3 = fusion(copy-done.962, copy-done.2560, copy-done.2558, fusion.950.remat5, ...(+8)), kind=kOutput, calls=fused_computation.950.clone.clone.clone
     Allocation type: HLO temp
     ==========================
  20. Size: 600.00M
     Operator: op_name="pmap(train_step)/jit(main)/jvp(FlaxAutoencoderKL)/FlaxAutoencoderKL.encode/encoder/down_blocks_1/resnets_0/conv1/conv_general_dilated[window_strides=(1, 1, 1) padding=((2, 0), (1, 1), (1, 1)) lhs_dilation=(1, 1, 1) rhs_dilation=(1, 1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 16, 192, 192, 128) rhs_shape=(3, 3, 3, 128, 256) precision=None preferred_element_type=None]" source_file="/home/ubuntu/.local/lib/python3.8/site-packages/flax/linen/linear.py" source_line=438
     Shape: f32[16,192,8,25,256]{4,2,3,1,0:T(8,128)}
     Unpadded size: 600.00M
     XLA label: fusion.1100.remat2 = fusion(copy-done.970, copy-done.2564, copy-done.2562, fusion.959.remat5, ...(+8)), kind=kOutput, calls=fused_computation.944.clone.clone
     Allocation type: HLO temp
     ==========================